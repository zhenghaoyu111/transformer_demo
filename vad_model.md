# 经过测试 其中Silero VAD效果最好，并且支持多种语言



## VAD 模型分类整理

### 1️⃣ 纯 VAD 模型（专门做 VAD）
这些模型专注于检测音频中的语音活动，只输出“语音/静音”标记。

| 模型名称              | 框架/来源            | 特点                                     |
| ----------------- | ---------------- | -------------------------------------- |
| **WebRTC VAD**    | C/C++ (Python封装) | 经典的基于能量+零交叉检测算法，速度快，超低延迟               |
| **Silero-VAD**    | PyTorch          | 基于轻量神经网络，支持长语音流式检测，开源                  |
| **FSMN-VAD**      | FunASR           | 深度神经网络 FSMN 结构，支持小样本训练                 |
| **MarbleNet VAD** | NVIDIA NeMo      | 专门设计的语音活动检测 CNN 网络                     |
| **TEN-VAD**       | 开源               | Transformer Encoder + Noise 抑制结构，支持端到端 |
| **Torch-VAD**     | PyTorch          | 简单轻量，适合边缘设备                            |


✅ **适用场景**：
- 电话语音检测
- ASR 前端静音过滤
- 低功耗设备实时检测


## 2️⃣ 内置 VAD 的 ASR 模型（语音识别模型内带 VAD）
部分语音识别模型内部集成了 VAD 功能，自动检测语音起止，无需单独 VAD。

| 模型名称                     | 框架/来源                 | VAD 功能描述              |
| ------------------------ | --------------------- | --------------------- |
| **Whisper (OpenAI)**     | PyTorch               | 内部有段落检测，能区分静音区域       |
| **FunASR (Paraformer)**  | FunASR                | 内置流式 VAD，长音频无需预剪切     |
| **Wav2Vec2.0 + VAD**     | Fairseq / Huggingface | 可配置前置 VAD 层进行预处理      |
| **NeMo ASR (QuartzNet)** | NVIDIA NeMo           | 流式推理 API 集成了简单 VAD 逻辑 |

✅ **适用场景**：
- 连续识别（会议/客服系统）
- 简化前后处理流程


### 3️⃣非专门 VAD，但可做 VAD 的模型（如 CTC 输出）
这类模型不是专门做 VAD，但通过概率分布或中间输出判断静音区段。

| 技术/模型                   | 说明                                       |
|-----------------------------|----------------------------------------------|
| **CTC-based ASR 模型**      | CTC blank 符号可用作静音检测                 |
| **Attention-based ASR**     | Attention 权重低的区域可能对应静音           |
| **Encoder 输出能量阈值法**  | 统计编码器层输出能量，低于阈值判为静音        |
| **HMM-GMM 声学模型**        | 经典 GMM VAD 可嵌入 HMM 解码中                |

✅ **适用场景**：
- 语音识别后端自动剪切静音
- 流式 ASR 输出流中判断语音段

### 总结：

| 类别               | 优点                | 缺点         |
| ---------------- | ----------------- | ---------- |
| **纯 VAD 模型**     | 专注检测、轻量、低延迟       | 不能识别语音内容   |
| **内置 VAD 的 ASR** | 无需单独调用、识别+VAD 一体化 | 难以单独调优 VAD |
| **CTC 等 VAD替代**  | 不用改网络结构、附加代价低     | 静音检测精度有限   |

## 主要流行的纯VAD模型

| 模型名称              | 开发方                     | 主要特性                                                                                                                                  | 性能表现                                                                          | 部署方式                                                                                                                         | 许可协议            | 适用场景                                       |
| ----------------- | ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- | --------------- | ------------------------------------------ |
| **Silero VAD**    | Silero Team             | - 支持 8kHz/16kHz<br>- 支持 PyTorch 和 ONNX<br>- 轻量级（\~2MB）<br>- 多语言支持（6000+）                                                              | - 在 AliMeeting、Earnings21、AISHELL-4 等多个数据集上表现优异<br>- 在 ESC-50 噪声数据集上准确率达 0.61 | - 支持 CPU 和 GPU<br>- 可在浏览器中运行（通过 ONNX）                                                                                        | MIT 许可          | - 呼叫中心<br>- 嵌入式设备<br>- 浏览器端应用              |
| **FSMN-VAD**      | FunASR / ModelScope     | - 基于 FSMN 架构<br>- 支持流式和离线模式<br>- 模型小巧（\~1.6MB）<br>- 训练于中文语料，具备抗噪能力                                                                    | - 在 Mac M1 Pro 上处理 70 秒音频仅需 0.6 秒<br>- 实时因子（RTF）可达 0.0077                     | - 支持 ONNX Runtime<br>- 可在 macOS 上运行                                                                                          | MIT 许可          | - 中文语音处理<br>- 嵌入式系统<br>- 实时语音识别            |
| **WebRTC VAD**    | Google                  | - 基于传统信号处理<br>- 支持 8kHz、16kHz、32kHz、48kHz 采样率<br>- 低延迟，适用于实时通信                                                                        | - 在多种数据集上表现稳定<br>- 但在噪声环境下性能有限                                                | - C/C++ 实现<br>- 可通过 Python 接口使用（如 py-webrtcvad）                                                                              | BSD 许可          | - 实时通信（VoIP）<br>- 嵌入式系统                    |
| **Cobra VAD**(闭源) | Picovoice               | - 基于深度学习<br>- 提供多平台 SDK（Python、Node.js、Android、iOS 等）<br>- 支持本地运行，保护用户隐私                                                              | - 准确率是 WebRTC VAD 的两倍<br>- 对背景噪声具有较强的鲁棒性                                      | - 跨平台支持<br>- 可嵌入移动设备和浏览器                                                                                                     | 商业许可（需联系销售）     | - 企业级应用<br>- 隐私敏感场景<br>- 移动和嵌入式设备          |
| **MarbleNet VAD** | NVIDIA                  | - 基于 CNN 的轻量级模型（91.5K 参数）<br>- 多语言支持（中、英、法、德、俄、西）<br>- 每 20ms 输出一次语音概率                                                                | - 训练时加入了白噪声和真实世界噪声扰动<br>- 在多种语言和噪声环境下表现良好                                     | - 支持 PyTorch 和 ONNX<br>- 可在多种平台部署                                                                                            | NVIDIA 开源模型许可协议 | - 多语言语音识别<br>- 嵌入式系统<br>- 噪声环境下的语音检测       |
| **TEN‑VAD**       | TEN‑framework           | - - 基于深度学习，专为企业级对话系统设计，具备高精度的帧级语音检测能力 <br>    - 检测延迟极低，远优于 WebRTC 和 Silero VAD，快速捕捉语音 → 非语音切换<br>    - 极轻量，库体积小（几十 KB 到几百 KB）、计算资源占用低 | - 与 Silero VAD 比延迟降低 ~32%；      <br>- 能检测动静切换几乎“零延迟”；<br>- 高噪声条件下准确率和误警率表现良好  | - - 提供跨平台支持：C 语言库覆盖 Linux/macOS/Windows/Android/iOS，同时提供 Python 绑定（Linux）及 WebAssembly（WASM）版本 <br>    - 支持 ONNX，便于嵌入各类引擎和设备 | Apache开源许可证     | -适合对延迟敏感的实时对话系统，如智能客服、语音助手、会议记录等，跨平台部署能力强。 |
| ~~torch_vad~~     | 这个vad只能消除语音前后的噪声，识别效果很差 |                                                                                                                                       |                                                                               |                                                                                                                              |                 |                                            |


### VAD 模型性能指标汇总

 1. 精确率 (Precision)
- **中文解释**: 检测为语音的帧中，实际是语音的比例（降低误报）。
- **公式**:  
   `Precision = TP / (TP + FP)`
  - **TP**: 真阳性（语音检测正确）  
  - **FP**: 假阳性（非语音误检为语音）

---

2. 召回率 (Recall) / 灵敏度 (Sensitivity)
- **中文解释**: 实际语音帧中，被检测出来的比例（降低漏报）。
- **公式**:  
  `Recall = TP / (TP + FN)`
  - **FN**: 假阴性（语音漏检）
  - **TN** (真阴性)：指实际为负类，模型也正确预测为负类的样本数。

---

 3. F1 分数 (F1 Score)
- **中文解释**: 精确率和召回率的调和平均，用于平衡二者。
- **公式**:  
   `F1 = 2 * (Precision * Recall) / (Precision + Recall)`



> 📝 **说明**:  
- VAD（语音活动检测）模型的目标是 **最大化 F1 。  
- 在实际应用中，**不同场景可能权衡 Precision 和 Recall**。

### 常用 VAD 数据集

 1.AliMeeting
- **中文名称**: AliMeeting 中文多说话人会议语音数据集
- **简介**: 面向远场会议场景的大规模多说话人语音数据集，适合 VAD、ASR 及说话人分离等任务的研究。
- **特点**:
    - 覆盖多种会议场景（远场麦克风阵列录制）
    - 包含约 110 小时高质量多说话人中文语音
    - 提供参考转录文本和扬声器标签
    - 适合多说话人识别、语音分离和降噪任务
- **链接**: [AliMeeting 数据集](https://openslr.org/109/)

 2.AISHELL‑4
- **中文名称**: AISHELL-4 会议场景多说话人中文语音数据集
- **简介**: 适用于会议语音识别、VAD 和多说话人分离任务的大规模中文数据集。
- **特点**:
    - 约 120 小时多说话人中文语音
    - 录制环境为真实会议场景（不同麦克风设置）
    - 包含完整转录和扬声器分离标签
    - 专注于多说话人场景下的语音任务
- **链接**: [AISHELL-4 数据集](https://openslr.org/111/)

3. Speech Activity Detection Datasets
- **中文名称**: 语音活动检测数据集
- **简介**: 专门为语音活动检测（VAD/SAD）任务设计的数据集，包含带标注的语音活动和静音段，适合用于训练和评估 VAD 模型。
- **特点**:
    - 覆盖多种语言、多种场景（安静、噪声、混响环境）
    - 提供高质量的语音活动（speech/silence）标注
    - 适合语音前端处理和 ASR 预处理任务
- **链接**: [Speech Activity Detection Datasets](https://huggingface.co/datasets/google/speech_activity_detection)

4. VAD 数据集 (KAIST)
- **中文名称**: KAIST 语音活动检测数据集
- **简介**: 由韩国 KAIST 提供的用于训练和评估语音活动检测（VAD）模型的数据集，包含不同噪声环境下的语音和静音标注，适用于 VAD/SAD 研究和模型优化。
- **特点**:
    - 包含多说话人、多种环境（安静、街道、室内等）
    - 高质量的语音活动标注，可直接用于帧级 VAD 模型训练
    - 支持生成语音活动概率序列（posterior）用于分析
    
- **链接**: [VAD Dataset (KAIST)](https://github.com/jtkim-kaist/VAD)





下面是我自己实际测试得到的数据：
运行环境：macos M4
使用的是公开标注的数据集：VAD 数据集 (KAIST)

| vad model     | TP    | FP    | FN    | TN    | Precision | Recall | F1     | 延迟     | 1s语音检测时间 |
| ------------- | ----- | ----- | ----- | ----- | --------- | ------ | ------ | ------ | -------- |
| Silero‑VAD v4 | 14687 | 1221  | 571   | 39897 | 0.9232    | 0.9626 | 0.9425 | 4.97s  | 48.25 ms |
| FSMN-VAD      | 448   | 681   | 14810 | 40437 | 0.3968    | 0.0294 | 0.0547 | 5.47s  | 54.70 ms |
| WebRTC VAD    | 12871 | 11443 | 3407  | 32413 | 0.5294    | 0.7907 | 0.6342 | 0.168s | 0.13 ms  |
| MarbleNet VAD | 27    | 11    | 690   | 175   | 0.7105    | 0.0377 | 0.0715 | 7.45s  | 11.21 ms |
| TEN-VAD       | 14047 | 40993 | 1177  | 118   | 0.2552    | 0.9227 | 0.3998 | 2.44s  | 12ms     |

融合数据集： **AliMeeting**    **AISHELL‑4（会议场景）**

| vad model     | TP    | FP    | FN    | TN    | Precision | Recall | F1     | 延迟     | 1s语音检测时间 |
| ------------- | ----- | ----- | ----- | ----- | --------- | ------ | ------ | ------ | -------- |
| Silero‑VAD v4 | 25187 | 13051 | 9602  | 27160 | 0.6587    | 0.7240 | 0.6898 | 6.58s  | 2.94ms   |
| FSMN-VAD      | 2865  | 410   | 31924 | 39801 | 0.8748    | 0.0824 | 0.1505 | 4.84s  | 14.93ms  |
| WebRTC VAD    | 16630 | 15151 | 20482 | 27737 | 0.5233    | 0.4481 | 0.4828 | 0.20s  | 0.09ms   |
| MarbleNet VAD | 243   | 387   | 326   | 244   | 0.3857    | 0.4271 | 0.4053 | 10.38s | 8ms      |
| TEN-VAD       | 34590 | 39797 | 199   | 374   | 0.4650    | 0.9943 | 0.6337 | 3.69s  | 8ms      |



### 模型加速问题
模型均使用cpu进行测试，因为mac不支持CUDA。如果使用mps的GPU加速小型的模型会造成传输数据时间长，反而导致整个程序运行时间比在cpu上运行时间长。

## 总结

## 🏆 综合排名（基于两个数据集）

| 综合排名 | VAD 模型            | 综合表现                                        |
| ---- | ----------------- | ------------------------------------------- |
| 🥇 1 | **Silero‑VAD v4** | 两个数据集均为 F1 最高，精度和召回均衡，适合离线和在线场景             |
| 🥈 2 | TEN-VAD           | 融合数据集 Recall 极高（漏检少），但 Precision 偏低，误检偏多    |
| 🥉 3 | WebRTC VAD        | 延迟极低（毫秒级），在实时性要求场景有优势，但 F1 不如 Silero‑VAD v4 |
| 4    | FSMN-VAD          | Precision 高但漏检严重，整体 F1 较低                   |
| 5    | MarbleNet VAD     | F1 最低，延迟最高，不适合实时应用                          |

## 结论：
✅ 如果你需要 **高精度+高召回**（比如 **离线批处理**）：

👉 **Silero‑VAD v4** 最稳，综合性能最佳。


✅ 如果需要 **低延迟实时处理**（比如 **电话语音、流式传输**）：

👉 **WebRTC VAD** 非常快（<1ms），但 F1 不如 Silero。


✅ 如果追求 **漏检少**（宁可误检）：

👉 **TEN-VAD** 召回率极高，适合 **高召回场景**。


## 比较一下带vad（Silero VAD）与不带vad的whisper效果

| 模型                 | WER(%) | 延迟（s） |
| ------------------ | ------ | ----- |
| whisper            | 10.2   | 1.55  |
| Silero vad+whisper | 9.4    | 1.32  |


### 结论
这组实验结果显示，**Silero VAD + Whisper** 相比单独使用 **Whisper** 在识别性能和速度上都有所提升：在语音识别中，VAD 有效去除了无声和噪声段，使得整体 **错词率（WER）降低了 0.8%**，平均延迟也减少了约 **0.23 秒**。这表明引入语音活动检测不仅提高了识别准确率，还提升了处理效率，尤其适合长语音或包含大量静音段的场景。